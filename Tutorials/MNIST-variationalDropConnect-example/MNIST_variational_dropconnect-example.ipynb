{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIDDEN MARKOV NEURAL NETWORK: Variational DropConnect example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test variational DropConnect as regularization criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "import BayesianNetworkmu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_load_mnist():\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = 'train-labels-idx1-ubyte.gz'\n",
    "    images_path = 'train-images-idx3-ubyte.gz'\n",
    "        \n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        lbpath.read(8)\n",
    "        buffer = lbpath.read()\n",
    "        labels = np.frombuffer(buffer, dtype=np.uint8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        imgpath.read(16)\n",
    "        buffer = imgpath.read()\n",
    "        images = np.frombuffer(buffer, \n",
    "                               dtype=np.uint8).reshape(\n",
    "            len(labels), 784).astype(np.float64)\n",
    " \n",
    "    return images, labels\n",
    "\n",
    "def train_mnist_preprocessing(x, y):\n",
    "    x = np.float64(x)/126\n",
    "#     np.save(\"train_mnist_preprocessed_data\", x)\n",
    "    y = np.int32(y)\n",
    "#     np.save(\"train_mnist_preprocessed_target\", y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files \"train-labels-idx1-ubyte.gz\" and \"train-images-idx3-ubyte.gz\" can be downloaded from:\n",
    "\n",
    "- http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_load_mnist()\n",
    "tr_x, tr_y = train_mnist_preprocessing(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(tr_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the hyper parameters for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters \n",
    "sample_size    = 50000\n",
    "minibatch_size = 128\n",
    "epocs          = 10 #600\n",
    "\n",
    "sliding = 0\n",
    "T = 1\n",
    "\n",
    "###########################################################\n",
    "# Set the network structure\n",
    "\n",
    "L = 4\n",
    "architecture = np.array([784, 400, 400, 10])\n",
    "\n",
    "alpha_k = 0.0\n",
    "sigma_k = np.exp(-0)\n",
    "c       = np.exp(6)\n",
    "p       = 0.25\n",
    "pi      = 0.5\n",
    "lr_c = 1e-3\n",
    "mc_c = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior score is tiny because we are plotting the score at the last epoch, which is scaled during training as suggest in (Blundell et al. 2015). The scaling factor is $\\frac{2^{M-i}}{2^{M}-i}$, where $i$ refer to the minibatch we are training on and $M$ refers to the total number of minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "960/24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1\n",
      "Prior 2.4951869940664257e-114. Loss 2.235873012648981\n",
      "Performance: 0.2561\n",
      "Prior 2.5411418150492362e-114. Loss 2.139645263095006\n",
      "Performance: 0.2982\n",
      "Prior 2.5745815171070378e-114. Loss 1.888837404794399\n",
      "Performance: 0.6318\n",
      "Prior 2.601015898045989e-114. Loss 1.3680846954740458\n",
      "Performance: 0.7358\n",
      "Prior 2.618342247542551e-114. Loss 0.8901065520978004\n",
      "Performance: 0.7887\n",
      "Prior 2.635756321586856e-114. Loss 0.779735812441807\n",
      "Performance: 0.823\n",
      "Prior 2.655706352765001e-114. Loss 0.7705914052691832\n",
      "Performance: 0.8546\n",
      "Prior 2.661508405460976e-114. Loss 0.39283432951064373\n",
      "Performance: 0.8677\n",
      "Prior 2.674510204888462e-114. Loss 0.608907315854538\n",
      "Performance: 0.8769\n",
      "Prior 2.6885693276614304e-114. Loss 0.788432992070009\n",
      "Performance: 0.8832\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Algorithm training\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "HMMNET = BayesianNetworkmu.torchHHMnet(architecture, alpha_k, sigma_k, c, pi, p, loss_function, sample_size, minibatch_size, epocs, T, sliding, workers = 1)\n",
    "\n",
    "HMMNET.forward_pass(tr_x[0:50000,:], tr_y[0:50000], tr_x[50000:60000,:], tr_y[50000:60000], lr_c, mc_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above simulation took around 15 min on a machine with CPU: 1.80GHz i7-8565U and Ram: 16Gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_mnist():\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = 't10k-labels-idx1-ubyte.gz'\n",
    "    images_path = 't10k-images-idx3-ubyte.gz'\n",
    "        \n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        lbpath.read(8)\n",
    "        buffer = lbpath.read()\n",
    "        labels = np.frombuffer(buffer, dtype=np.uint8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        imgpath.read(16)\n",
    "        buffer = imgpath.read()\n",
    "        images = np.frombuffer(buffer, \n",
    "                               dtype=np.uint8).reshape(\n",
    "            len(labels), 784).astype(np.float64)\n",
    " \n",
    "    return images, labels\n",
    "\n",
    "def test_mnist_preprocessing(x, y):\n",
    "    x = np.float64(x)/126\n",
    "    #np.save(\"test_mnist_preprocessed_data\", x)\n",
    "    y = np.int32(y)\n",
    "    #np.save(\"test_mnist_preprocessed_target\", y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = test_load_mnist()\n",
    "te_x, te_y = test_mnist_preprocessing(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: 0.8827\n"
     ]
    }
   ],
   "source": [
    "y_predicted     = np.zeros(len(te_y))\n",
    "test_performance =0\n",
    "\n",
    "for t in range(1,2):\n",
    "    output           = HMMNET.model_list[t].performance( torch.tensor( te_x, dtype = torch.float64 ) )\n",
    "    output_softmax   = F.softmax(output, dim=1)\n",
    "\n",
    "    y_predicted = np.array( range(0, 10) )[ np.argmax( output_softmax.data.numpy(), 1 ) ]\n",
    "\n",
    "    test_performance = sum(te_y == y_predicted)/len(te_y)\n",
    "    print(\"Performance: \"+ str(test_performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
